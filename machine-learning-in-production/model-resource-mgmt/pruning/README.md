# Pruning
Increase efficiency of model

Pruning in Deep learning was biologically inspired

Reduce size through compress and removing connections 

Lower parameter count in the network

## Model sparsity
Larger models = more memory = less efficieny
Resnet50 == 25 million connections

Sparse models = less memory = more efficient

larger models tend to overfit

## weights pruning 

"Optimal brain damage" == Yan L.

## Benefits
1. disk compression (better storage or transmission)
1. Gain speedups  in CPU and ML Accelerators
1. Used in tandem with quantization to get compound benefits


