# Knowledge Distillation

Ways to optimize the implementation of models to make them more efficient. 
But you can also try to capture or distill the knowledge that has been learned by a model, in a more of compact model by using a different style of training. 
This is known as knowledge distillation.

But what happens if you want to deploy a model that is relatively complex and significantly large in size? '
Would merely reducing the number of parameters help with deployment? 
Or would reducing the model complexity help you deploy these larger, more powerful models? 

Models tend to become larger and more complex as they try to capture more information or knowledge in order to learn complex tasks. 
But if we can express or represent this learning more efficiently, we might be able to create smaller models that are equivalent to these larger, more complex models.

GoogleNet is a large complex model that cannot fit on most devices, like phones.

## What does it do?
1. Duplicates the performance of a complex model in a simplier model
1. Start with a complex model or ensemble that becomes a teacher   
1. Create a/many simple 'student model' that learns from a complex 'teacher model'


## Techniques
- Training objective of the teacher and student differ
- Teach 'normal training' - The teacher will be trained first using a standard objective function that seeks to maximize the accuracy or a similar metric of the model. This is normal model training.
    - max. the actual metric
- Student 'knowledge transfer' - The student then seeks transferable knowledge. It uses that objective function that seeks to match the probability distribution of the predictions of the teacher. Notice that the student is not just learning the teacher's predictions, but the probabilities of the predictions. The probabilities of the predictions of the teacher form soft targets, which provide more information about the knowledge learned by the teacher than the resulting predictions themselves.
    - matches p-distribution of the teachers predictions to form soft targets
    - 'soft targets' tell us about the knowledge learned by the teacher

## Dark Knowledge
transferring knowledge to the student is "dark knowledge"

- The way knowledge distillation works is that you transfer knowledge from the teacher to the student by minimizing a loss function, in which the target is the distribution of class probabilities predicted by the teacher model. What happens here is that the teacher models logits form the input to the final softmax layer, which is often used since they provide more information about the probabilities of all target classes for each example. 
- Hinton, Vinyals, and Dean, introduced the concept of a softmax temperature. By raising the temperature in the objective functions of the student and teacher, you can improve the softness of the teacher's distribution. 
- The authors also found another interesting behavior. It turns out that distilled models are able to produce the correct labels in addition to the teacher's soft targets. That means that you can calculate the standard loss between the student's predicted class probabilities and the ground truth labels. These are known as hard labels or targets. 
- In the formula here, the probability p of class i is calculated from the logits z as shown
- T simply refers to the temperature parameter. 
- When t is 1, you get the standard softmax function. 
- But as T starts growing, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes the teacher found more similar to the predicted class. The authors call this the dark knowledge embedded in the teacher model. It is this dark knowledge that you are transferring to the student model in the distillation process.

- The objective here is to make the distribution over the classes predicted by the student as close as possible to the teacher. When computing the loss function versus the teacher's soft targets, we use the same value of T to compute the softmax on the student's logits. This loss is the distillation loss.

## Approaches
1. Weigh objective (student and teach) and combine during backpropagation
1. Compare distrubitions of the predictions** - more wildly used - (student and teacher) using KL divergence

## Real World
In the real world though, people are more interested in deploying a low-resource model with close to state of the art results, but a lot smaller and a lot faster. 
That's why Hugging Face created DistilBERT. 
- A distilled version of BERT which uses 40 percent fewer parameter
- runs 60 percent faster while preserving 97 percent of BERT's performance as measured on the GLUE language understanding benchmark. 
Basically, it's a smaller version of BERT where the token type embeddings and the polar layer typically used for the next sentence classification task are removed. 
  - To create DistilBERT, the researchers at Hugging Face applied knowledge distillation to BERT, and hence the name DistilBERT. They kept the rest of the architecture identical while reducing the number of layers.

Reference notebook: https://keras.io/examples/vision/knowledge_distillation/https://keras.io/examples/vision/knowledge_distillation/

# Two-stage model knowledge distillation "TMKD"
A unique aspect of TMKD is that it uses a multi teacher distillation task for student model pre training to boost model performance. To analyze the impact of pre training the authors evaluated two models. The first one TKD is a three layer bert based model which is first trained using basic knowledge distillation pre training on the com Q A dataset. And then fine tune on a task specific corpus by using only one teacher for each task. The second model is a traditional knowledge distillation model which is again the same model but without the distillation pre training stage. 

sources: https://arxiv.org/pdf/1503.02531.pdfhttps://arxiv.org/pdf/1503.02531.pdf